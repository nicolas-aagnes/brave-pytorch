{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nico/github/brave-pytorch/.venv/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 22    \n",
      "--------------------------------\n",
      "22        Trainable params\n",
      "0         Non-trainable params\n",
      "22        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/Users/nico/github/brave-pytorch/.venv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 100/100 [00:00<00:00, 456.49it/s, loss=0.948, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 22    \n",
      "--------------------------------\n",
      "22        Trainable params\n",
      "0         Non-trainable params\n",
      "22        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: 100%|██████████| 100/100 [00:00<00:00, 284.12it/s, loss=0.948, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type     | Params\n",
      "----------------------------------------\n",
      "0 | modelA     | MyModelA | 22    \n",
      "1 | modelB     | MyModelB | 22    \n",
      "2 | classifier | Linear   | 10    \n",
      "----------------------------------------\n",
      "10        Trainable params\n",
      "44        Non-trainable params\n",
      "54        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 100/100 [00:00<00:00, 348.43it/s, loss=0.943, v_num=2]\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class MyModelA(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim = 10):\n",
    "        super(MyModelA, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(hidden_dim, 2)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\n",
    "        return optimizer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        return F.mse_loss(self.forward(x), y)\n",
    "    \n",
    "class MyModelB(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim = 10):\n",
    "        super(MyModelB, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(hidden_dim, 2)\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\n",
    "        return optimizer\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        return F.mse_loss(self.forward(x), y)\n",
    "\n",
    "class MyEnsemble(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                modelA_hparams, modelB_hparams, \n",
    "                modelA_params = None, modelB_params = None):\n",
    "        super(MyEnsemble, self).__init__()\n",
    "        self.modelA = MyModelA(**modelA_hparams)\n",
    "        self.modelB = MyModelB(**modelA_hparams)\n",
    "\n",
    "        if modelA_params:\n",
    "            self.modelA.load_state_dict({k: v[\"value\"].reshape(v[\"shape\"])\n",
    "                                        for k, v in modelA_params.items()})\n",
    "        if modelB_params:\n",
    "            self.modelB.load_state_dict({k: v[\"value\"].reshape(v[\"shape\"])\n",
    "                                        for k, v in modelB_params.items()})\n",
    "\n",
    "        self.modelA.freeze()\n",
    "        self.modelB.freeze()\n",
    "        self.classifier = torch.nn.Linear(4, 2)\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"modelA_params\", \"modelB_params\"])\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\n",
    "        return optimizer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.modelA(x)\n",
    "        x2 = self.modelB(x)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        return F.mse_loss(self.forward(x), y)\n",
    "\n",
    "dl = DataLoader(TensorDataset(torch.randn(1000, 10), \n",
    "                            torch.randn(1000, 2)), \n",
    "                batch_size = 10)\n",
    "\n",
    "modelA = MyModelA(10)\n",
    "modelB = MyModelB(10)\n",
    "\n",
    "# pretrained modelA and modelB\n",
    "trainerA = pl.Trainer(gpus = 0, max_epochs = 5, progress_bar_refresh_rate = 50)\n",
    "trainerA.fit(modelA, dl)\n",
    "trainerB = pl.Trainer(gpus = 0, max_epochs = 5, progress_bar_refresh_rate = 50)\n",
    "trainerB.fit(modelB, dl)\n",
    "\n",
    "# Reshape parameters/weights such that it is 1D\n",
    "modelA_params = {k: {\"shape\": v.shape,\"value\": torch.flatten(v)} \n",
    "                for k, v in modelA.state_dict().items()}\n",
    "modelB_params = {k: {\"shape\": v.shape,\"value\": torch.flatten(v)} \n",
    "                for k, v in modelB.state_dict().items()}\n",
    "modelA_hparams = modelA.hparams\n",
    "modelB_hparams = modelB.hparams\n",
    "\n",
    "# modelA and modelB contain pretrained weights\n",
    "model = MyEnsemble(modelA_hparams, modelB_hparams, \n",
    "                modelA_params, modelB_params)\n",
    "\n",
    "trainer = pl.Trainer(gpus = 0, max_epochs = 5, progress_bar_refresh_rate = 50)\n",
    "trainer.fit(model, dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "796b22f4fed15232e63d4dd6a4b5fa532c0009af6a2209a361b962eb0bd96613"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
